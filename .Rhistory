kde_data_lists <- list(
rushing_global = create_kde_data(analysis_data, "rushing_yards", "median_rushing_yards"),
rushing_season = create_kde_data(last_season_data, "rushing_yards", "median_rushing_yards"),
passing_global = create_kde_data(analysis_data, "passing_yards", "median_passing_yards"),
passing_season = create_kde_data(last_season_data, "passing_yards", "median_passing_yards"),
receiving_global = create_kde_data(analysis_data, "receiving_yards", "median_receiving_yards"),
receiving_season = create_kde_data(last_season_data, "receiving_yards", "median_receiving_yards")
)
# --- CORRECTED & MORE ROBUST KDE COMPUTATION ---
all_kde_results <- purrr::map(kde_data_lists, function(data_list) {
purrr::map(data_list, function(samples) {
if (length(samples) < MIN_KDE_SAMPLES || sd(samples, na.rm = TRUE) == 0) return(NULL)
# Try the sophisticated bw.SJ method first
bw <- suppressWarnings(tryCatch(bw.SJ(samples), error = function(e) NA))
# If it fails, fall back to the simpler bw.nrd0 method
if (is.na(bw) || !is.finite(bw) || bw <= 0) {
bw <- bw.nrd0(samples)
}
list(density_object = density(samples, bw = bw))
}) %>% purrr::compact()
})
#################################################################
## Step 7: Calculate Player-Specific KDE Distributions
#################################################################
cat("--- Step 7: Calculating Player-Specific KDE Distributions ---\n")
# --- Helper functions for KDE ---
calculate_kde_cdf_at_point <- function(d_obj, pt) approxfun(d_obj$x, cumsum(d_obj$y * (d_obj$x[2]-d_obj$x[1])), rule = 2)(pt)
calculate_interp_from_m1 <- function(m1, k1, k2) {
x_range <- range(k1$density_object$x, k2$density_object$x); common_x <- seq(x_range[1], x_range[2], length.out = INTERP_POINTS)
y1 <- approxfun(k1$density_object$x, k1$density_object$y, rule=2)(common_x)
y2 <- approxfun(k2$density_object$x, k2$density_object$y, rule=2)(common_x)
interp_y <- (m1 * y1) + ((1 - m1) * y2); interp_y <- interp_y / (sum(interp_y) * (common_x[2]-common_x[1]))
tibble(yards = common_x, cdf = cumsum(interp_y * (common_x[2]-common_x[1])))
}
calibrate_cdf <- function(cdf_data, target_x, target_p) {
cdf_func <- approxfun(cdf_data$yards, cdf_data$cdf, rule=2)
Fx <- cdf_func(target_x)
## FIX: Add guard for division-by-zero in extreme tails
if (is.na(Fx) || Fx <= 1e-9 || Fx >= 1 - 1e-9) {
return(cdf_data)
}
h <- function(u) { if_else(u <= Fx, (target_p/Fx)*u, target_p + ((1-target_p)/(1-Fx))*(u-Fx)) }
cdf_data %>% mutate(cdf = h(cdf))
}
# --- Process KDE scenarios ---
kde_results <- pmap_dfr(player_scenarios, function(...) {
scen <- list(...); yt <- scen$yardage_type; tl <- scen$target_line; tp <- scen$target_prob
p_bin <- floor(tl / SIZE_BINS) * SIZE_BINS; l_bin <- if(tl >= p_bin+(SIZE_BINS/2)) p_bin else p_bin-SIZE_BINS; u_bin <- l_bin + SIZE_BINS
process_source <- function(source) {
k_list <- all_kde_results[[paste(yt, source, sep="_")]]; k1 <- k_list[[as.character(l_bin)]]; k2 <- k_list[[as.character(u_bin)]]
if(is.null(k1) || is.null(k2)) return(list(pf = NULL, ci = NULL))
c1 <- calculate_kde_cdf_at_point(k1$density_object, tl); c2 <- calculate_kde_cdf_at_point(k2$density_object, tl)
m1_prob <- if(abs(c1-c2)<1e-9) NA else (tp-c2)/(c1-c2)
pf <- if(!is.na(m1_prob) && m1_prob>=0 && m1_prob<=1) calculate_interp_from_m1(m1_prob, k1, k2) else NULL
m1_lin <- (u_bin-tl)/SIZE_BINS; ci <- calibrate_cdf(calculate_interp_from_m1(m1_lin, k1, k2), tl, tp)
list(pf=pf, ci=ci)
}
g_res <- process_source("global"); s_res <- process_source("season")
blend <- function(cdf_g, cdf_s) {
if(is.null(cdf_g) && is.null(cdf_s)) return(NULL)
w_g <- if(is.null(cdf_s)) 1.0 else GLOBAL_WEIGHT; w_s <- if(is.null(cdf_g)) 1.0 else SEASON_WEIGHT
ref_cdf <- if(!is.null(cdf_g)) cdf_g else cdf_s; common_x <- ref_cdf$yards
y_g <- if(!is.null(cdf_g)) approxfun(cdf_g$yards,cdf_g$cdf,rule=2)(common_x) else 0
y_s <- if(!is.null(cdf_s)) approxfun(cdf_s$yards,cdf_s$cdf,rule=2)(common_x) else 0
tibble(yards=common_x, cdf=(w_g*y_g)+(w_s*y_s))
}
tibble(
player_name = scen$player_name, yardage_type = yt, target_line = tl,
KDE_Prob_Forced = list(blend(g_res$pf, s_res$pf)),
KDE_Calibrated_Interp = list(blend(g_res$ci, s_res$ci))
)
}) %>% pivot_longer(cols = starts_with("KDE"), names_to="model_type", values_to="cdf_data") %>% filter(!sapply(cdf_data, is.null))
# =================================================================
# =================================================================
## ðŸ“‹ PART C: CONSOLIDATE RESULTS & GENERATE OUTPUTS
# =================================================================
# =================================================================
cat("\n\n--- PART C: Consolidating All Results and Generating Outputs ---\n")
#################################################################
## Step 8: Consolidate All Model Results
#################################################################
cat("--- Step 8: Consolidating All Model Results into One Tibble ---\n")
all_model_results <- bind_rows(qr_results, kde_results) %>%
# FIX: Remove any rows where CDF generation failed for any model.
# This handles cases where the target_line is too extreme.
filter(!sapply(cdf_data, is.null))
print(all_model_results)
#################################################################
## Step 9: Generate Final Odds Tables (Revised)
#################################################################
cat("\n--- Step 9: Generating Final Odds Tables for Each Model ---\n")
# --- 9.1: Generate and Print Separate Tables for Each Model ---
# (Helper function is unchanged)
generate_final_table <- function(results_df, model_filter) {
# First, filter the incoming data for the specific model type
model_data <- results_df %>%
filter(model_type == model_filter)
# --- NEW FIX: Guard Clause ---
# If filtering results in an empty (0-row) data frame, stop and
# return a correctly structured empty table to prevent a crash.
if (nrow(model_data) == 0) {
return(
tibble(
player_name = character(),
yardage_type = character(),
yardage_bin = character(),
prob_under = numeric(),
prob_over = numeric()
)
)
}
# If data exists, proceed with the original logic
model_data %>%
mutate(
milestones = map(yardage_type, ~ CDF_MILESTONES[[.x]]),
# Calculate the CDF value at each milestone (the upper bound of each bin)
probs_at_milestones = map2(cdf_data, milestones, function(cdf_tbl, m) {
prob_func <- approxfun(cdf_tbl$yards, cdf_tbl$cdf, rule = 2, f = 0, yleft = 0, yright = 1)
tibble(milestone = m, cdf_at_bin_end = prob_func(m))
})
) %>%
select(player_name, yardage_type, probs_at_milestones) %>%
tidyr::unnest(cols = probs_at_milestones) %>%
filter(milestone > 0) %>%
group_by(player_name, yardage_type) %>%
# CORRECTED LOGIC: Use lag() to get the probability at the START of the bin
mutate(
# prob_under is the CDF at the start of the bin
prob_under = lag(cdf_at_bin_end, default = 0),
# prob_over is 1 minus the CDF at the end of the bin
prob_over = 1 - cdf_at_bin_end,
# Create the label for the bin
yardage_bin = paste(lag(milestone, default = 0), "-", milestone, "yds")
) %>%
ungroup() %>%
# Select the final, desired columns
select(player_name, yardage_type, yardage_bin, prob_under, prob_over)
}
# --- Generate and print the three tables ---
table_qr <- generate_final_table(all_model_results, "QR_Ensembled")
table_kde_cal <- generate_final_table(all_model_results, "KDE_Calibrated_Interp")
table_kde_pf <- generate_final_table(all_model_results, "KDE_Prob_Forced")
cat("\n\n--- Odds Table: QR Ensembled Model ---\n")
print(knitr::kable(table_qr %>% mutate(across(where(is.numeric), ~round(.x, 2))), caption = "Odds (QR Ensembled Model)"))
cat("\n\n--- Odds Table: KDE Calibrated Interpolation Model ---\n")
print(knitr::kable(table_kde_cal %>% mutate(across(where(is.numeric), ~round(.x, 2))), caption = "Odds (KDE Calibrated Model)"))
cat("\n\n--- Odds Table: KDE Probability Forced Model ---\n")
print(knitr::kable(table_kde_pf %>% mutate(across(where(is.numeric), ~round(.x, 2))), caption = "Odds (KDE Prob-Forced Model)"))
# --- 9.2: NEW - Create and Print a Single Unified Summary Table ---
# Rename columns in each table to prepare for joining
qr_renamed <- table_qr %>% rename_with(~ paste0(.x, "_QR"), .cols = starts_with("prob"))
kde_cal_renamed <- table_kde_cal %>% rename_with(~ paste0(.x, "_KDE_Cal"), .cols = starts_with("prob"))
kde_pf_renamed <- table_kde_pf %>% rename_with(~ paste0(.x, "_KDE_PF"), .cols = starts_with("prob"))
# Join the three tables together
unified_summary_table <- qr_renamed %>%
full_join(kde_cal_renamed, by = c("player_name", "yardage_type", "yardage_bin")) %>%
full_join(kde_pf_renamed, by = c("player_name", "yardage_type", "yardage_bin")) %>%
# CORRECTED: Reorder columns to group by model type
select(
player_name, yardage_type, yardage_bin,
prob_under_QR, prob_over_QR,
prob_under_KDE_Cal, prob_over_KDE_Cal,
prob_under_KDE_PF, prob_over_KDE_PF
) %>%
# Round all numeric columns for a clean final table
mutate(across(where(is.numeric), ~ round(.x, 2)))
cat("\n\n--- UNIFIED Summary Table: All Models Compared ---\n")
print(player_scenarios)
print(knitr::kable(unified_summary_table, caption = "Unified Model Comparison"))
# -------------------------------
# 4. Process all events IN BATCHES to save memory
# -------------------------------
cat("Processing", nrow(events), "event files in batches...\n")
# -------------------------------
# 0. Load libraries
# -------------------------------
library(data.table)
library(jsonlite)
library(dplyr)
# -------------------------------
# 1. Load and filter match files (No changes needed)
# -------------------------------
matches <- list.files(path = 'data/matches', full.names = TRUE, recursive = TRUE)
match_list <- lapply(matches, function(fn) as.data.table(read_json(fn, simplifyVector = TRUE)))
matches_dt <- rbindlist(match_list, fill = TRUE)
matches_dt <- matches_dt[!duplicated(match_id), ]
# -------------------------------
# 0. Load libraries
# -------------------------------
library(data.table)
library(jsonlite)
library(dplyr)
# -------------------------------
# 1. Load and filter match files (No changes needed)
# -------------------------------
matches <- list.files(path = 'data/matches', full.names = TRUE, recursive = TRUE)
match_list <- lapply(matches, function(fn) as.data.table(read_json(fn, simplifyVector = TRUE)))
matches_dt <- rbindlist(match_list, fill = TRUE)
# âœ… FIX: Use the recommended data.table 'unique' function
matches_dt <- unique(matches_dt, by = "match_id")
top_leagues <- c('1. Bundesliga','Premier League','Serie A','La Liga','Ligue 1')
matches_dt <- matches_dt[competition.competition_name %in% top_leagues]
setwd("~/Desktop/github/open-data")
# -------------------------------
# 0. Load libraries
# -------------------------------
library(data.table)
library(jsonlite)
library(dplyr)
# -------------------------------
# 1. Load and filter match files
# -------------------------------
matches <- list.files(path = 'data/matches', full.names = TRUE, recursive = TRUE)
match_list <- lapply(matches, function(fn) as.data.table(read_json(fn, simplifyVector = TRUE)))
matches_dt <- rbindlist(match_list, fill = TRUE)
# Use the recommended data.table 'unique' function
matches_dt <- unique(matches_dt, by = "match_id")
top_leagues <- c('1. Bundesliga','Premier League','Serie A','La Liga','Ligue 1')
# âœ… FIX: Explicitly reference the column inside the data.table
matches_dt <- matches_dt[matches_dt$competition.competition_name %in% top_leagues]
# -------------------------------
# 2. Load and filter event files (No changes needed)
# -------------------------------
events <- list.files(path = 'data/events', full.names = TRUE, recursive = TRUE)
events <- data.frame(fn = events)
events$match_id <- gsub(".json", "", basename(events$fn), fixed = TRUE)
events <- events[events$match_id %in% matches_dt$match_id, ]
# 3. Define REFINED robust function to extract events
# -------------------------------
get_events <- function(fn) {
# Skip empty files
if (file.size(fn) < 2) return(NULL) # Check for more than just empty brackets []
# IMPROVEMENT: Directly parse to a data.table (much faster)
dt <- as.data.table(read_json(fn, simplifyVector = TRUE))
if (nrow(dt) == 0) return(NULL)
# Flatten nested fields
if ("shot" %in% names(dt)) {
dt <- cbind(dt, dt$shot); dt[, shot := NULL]
}
if ("pass" %in% names(dt)) {
dt <- cbind(dt, dt$pass); dt[, pass := NULL]
}
if ("foul_committed" %in% names(dt)) {
dt <- cbind(dt, dt$foul_committed); dt[, foul_committed := NULL]
}
if ("bad_behaviour" %in% names(dt)) {
if ("card" %in% names(dt$bad_behaviour)) dt <- cbind(dt, dt$bad_behaviour$card)
dt[, bad_behaviour := NULL]
}
# IMPROVEMENT: Safely unify card columns using fcoalesce
# This creates the new 'card.name' column
dt[, card.name := fcoalesce(foul_committed.card.name, bad_behaviour.card.name, name)]
# Filter relevant events
dt <- dt[
(type.name == "Shot") |
(pass.type.name == "Corner") |
(type.name == "Own Goal For") |
(!is.na(card.name) & card.name %in% c("Yellow Card", "Red Card")) |
(type.name == "Foul Committed") |
(type.name %in% c("Half End", "Half Start"))
]
if (nrow(dt) == 0) return(NULL)
# Keep only desired columns
cols_to_keep <- c(
"id","index","period","timestamp","minute","second","match_id",
"type.id","type.name","possession_team.name","play_pattern.name",
"team.id","team.name","pass.type.name","card.name",
"outcome.name","type.name","penalty","statsbomb_xg"
)
# Add match_id and select columns
dt[, match_id := as.integer(gsub(".json", "", basename(fn), fixed = TRUE))]
dt <- dt[, intersect(cols_to_keep, names(dt)), with = FALSE]
return(dt)
}
# -------------------------------
# 4. Process all events IN BATCHES to save memory
# -------------------------------
cat("Processing", nrow(events), "event files in batches...\n")
# Define the size of each batch
batch_size <- 50
# Split the file list into a list of batches
file_batches <- split(events$fn, ceiling(seq_along(events$fn) / batch_size))
# Loop through each batch of file paths
for (i in seq_along(file_batches)) {
cat("Processing batch", i, "of", length(file_batches), "...\n")
# Get the file paths for the current batch
current_batch_files <- file_batches[[i]]
# Use lapply on just this small batch
batch_list <- lapply(current_batch_files, get_events)
batch_list <- batch_list[!sapply(batch_list, is.null)]
if (length(batch_list) > 0) {
batch_dt <- rbindlist(batch_list, fill = TRUE)
# Save the result of this batch to a CSV file
fwrite(batch_dt,
file = "all_events.csv",
append = file.exists("all_events.csv"),
col.names = !file.exists("all_events.csv"))
}
# Optional: Clean up memory before the next batch
rm(batch_list, batch_dt)
gc()
}
# -------------------------------
# 3. Define the FINAL, MOST ROBUST function to extract events
# -------------------------------
get_events <- function(fn) {
# Skip empty files
if (file.size(fn) < 2) return(NULL) # Check for more than just empty brackets []
# Directly parse to a data.table (much faster)
dt <- as.data.table(read_json(fn, simplifyVector = TRUE))
if (nrow(dt) == 0) return(NULL)
# Flatten nested fields
if ("shot" %in% names(dt)) {
dt <- cbind(dt, dt$shot); dt[, shot := NULL]
}
if ("pass" %in% names(dt)) {
dt <- cbind(dt, dt$pass); dt[, pass := NULL]
}
if ("foul_committed" %in% names(dt)) {
dt <- cbind(dt, dt$foul_committed); dt[, foul_committed := NULL]
}
if ("bad_behaviour" %in% names(dt)) {
# The 'card' object inside 'bad_behaviour' contains a column just named 'name'
if ("card" %in% names(dt$bad_behaviour)) dt <- cbind(dt, dt$bad_behaviour$card)
dt[, bad_behaviour := NULL]
}
# âœ… THE FIX: Robustly create the unified 'card.name' column
# 1. Define all possible names for card columns
potential_card_cols <- c("foul_committed.card.name", "bad_behaviour.card.name", "name")
# 2. Find which of these columns actually exist in the current data table
existing_card_cols <- intersect(potential_card_cols, names(dt))
# 3. If at least one card column exists, combine them using fcoalesce.
#    Otherwise, create an empty card.name column.
if (length(existing_card_cols) > 0) {
# .SDcols tells data.table to apply fcoalesce only to the existing columns
dt[, card.name := fcoalesce(.SD), .SDcols = existing_card_cols]
} else {
dt[, card.name := NA_character_]
}
# Filter relevant events
dt <- dt[
(type.name == "Shot") |
(pass.type.name == "Corner") |
(type.name == "Own Goal For") |
(!is.na(card.name) & card.name %in% c("Yellow Card", "Red Card")) |
(type.name == "Foul Committed") |
(type.name %in% c("Half End", "Half Start"))
]
if (nrow(dt) == 0) return(NULL)
# Keep only desired columns
cols_to_keep <- c(
"id","index","period","timestamp","minute","second","match_id",
"type.id","type.name","possession_team.name","play_pattern.name",
"team.id","team.name","pass.type.name","card.name",
"outcome.name","type.name","penalty","statsbomb_xg"
)
# Add match_id and select columns
dt[, match_id := as.integer(gsub(".json", "", basename(fn), fixed = TRUE))]
dt <- dt[, intersect(cols_to_keep, names(dt)), with = FALSE]
return(dt)
}
# -------------------------------
# 4. Process all events IN BATCHES to save memory
# -------------------------------
cat("Processing", nrow(events), "event files in batches...\n")
# Define the size of each batch
batch_size <- 50
# Split the file list into a list of batches
file_batches <- split(events$fn, ceiling(seq_along(events$fn) / batch_size))
# Loop through each batch of file paths
for (i in seq_along(file_batches)) {
cat("Processing batch", i, "of", length(file_batches), "...\n")
# Get the file paths for the current batch
current_batch_files <- file_batches[[i]]
# Use lapply on just this small batch
batch_list <- lapply(current_batch_files, get_events)
batch_list <- batch_list[!sapply(batch_list, is.null)]
if (length(batch_list) > 0) {
batch_dt <- rbindlist(batch_list, fill = TRUE)
# Save the result of this batch to a CSV file
fwrite(batch_dt,
file = "all_events.csv",
append = file.exists("all_events.csv"),
col.names = !file.exists("all_events.csv"))
}
# Optional: Clean up memory before the next batch
rm(batch_list, batch_dt)
gc()
}
setwd("~/Desktop/github/open-data")
setwd("~/Desktop/github/open-data")
# -------------------------------
# 0. Load libraries
# -------------------------------
library(data.table)
library(jsonlite)
library(dplyr)
# -------------------------------
# 1. Load and filter match files
# -------------------------------
matches <- list.files(path = 'data/matches', full.names = TRUE, recursive = TRUE)
match_list <- lapply(matches, function(fn) as.data.table(read_json(fn, simplifyVector = TRUE)))
matches_dt <- rbindlist(match_list, fill = TRUE)
# Use the recommended data.table 'unique' function
matches_dt <- unique(matches_dt, by = "match_id")
top_leagues <- c('1. Bundesliga','Premier League','Serie A','La Liga','Ligue 1')
# âœ… FIX: Explicitly reference the column inside the data.table
matches_dt <- matches_dt[matches_dt$competition.competition_name %in% top_leagues]
# -------------------------------
# 2. Load and filter event files (No changes needed)
# -------------------------------
events <- list.files(path = 'data/events', full.names = TRUE, recursive = TRUE)
events <- data.frame(fn = events)
events$match_id <- gsub(".json", "", basename(events$fn), fixed = TRUE)
events <- events[events$match_id %in% matches_dt$match_id, ]
# -------------------------------
# 3. Define the FINAL, MOST ROBUST function to extract events
# -------------------------------
get_events <- function(fn) {
# Skip empty files
if (file.size(fn) < 2) return(NULL) # Check for more than just empty brackets []
# Directly parse to a data.table (much faster)
dt <- as.data.table(read_json(fn, simplifyVector = TRUE))
if (nrow(dt) == 0) return(NULL)
# Flatten nested fields
if ("shot" %in% names(dt)) {
dt <- cbind(dt, dt$shot); dt[, shot := NULL]
}
if ("pass" %in% names(dt)) {
dt <- cbind(dt, dt$pass); dt[, pass := NULL]
}
if ("foul_committed" %in% names(dt)) {
dt <- cbind(dt, dt$foul_committed); dt[, foul_committed := NULL]
}
if ("bad_behaviour" %in% names(dt)) {
# The 'card' object inside 'bad_behaviour' contains a column just named 'name'
if ("card" %in% names(dt$bad_behaviour)) dt <- cbind(dt, dt$bad_behaviour$card)
dt[, bad_behaviour := NULL]
}
# âœ… THE FIX: Robustly create the unified 'card.name' column
# 1. Define all possible names for card columns
potential_card_cols <- c("foul_committed.card.name", "bad_behaviour.card.name", "name")
# 2. Find which of these columns actually exist in the current data table
existing_card_cols <- intersect(potential_card_cols, names(dt))
# 3. If at least one card column exists, combine them using fcoalesce.
#    Otherwise, create an empty card.name column.
if (length(existing_card_cols) > 0) {
# .SDcols tells data.table to apply fcoalesce only to the existing columns
dt[, card.name := fcoalesce(.SD), .SDcols = existing_card_cols]
} else {
dt[, card.name := NA_character_]
}
# Filter relevant events
dt <- dt[
(type.name == "Shot") |
(pass.type.name == "Corner") |
(type.name == "Own Goal For") |
(!is.na(card.name) & card.name %in% c("Yellow Card", "Red Card")) |
(type.name == "Foul Committed") |
(type.name %in% c("Half End", "Half Start"))
]
if (nrow(dt) == 0) return(NULL)
# Keep only desired columns
cols_to_keep <- c(
"id","index","period","timestamp","minute","second","match_id",
"type.id","type.name","possession_team.name","play_pattern.name",
"team.id","team.name","pass.type.name","card.name",
"outcome.name","type.name","penalty","statsbomb_xg"
)
# Add match_id and select columns
dt[, match_id := as.integer(gsub(".json", "", basename(fn), fixed = TRUE))]
dt <- dt[, intersect(cols_to_keep, names(dt)), with = FALSE]
return(dt)
}# -------------------------------
# 4. Process all events IN BATCHES to save memory
# -------------------------------
cat("Processing", nrow(events), "event files in batches...\n")
# Define the size of each batch
batch_size <- 50
# Split the file list into a list of batches
file_batches <- split(events$fn, ceiling(seq_along(events$fn) / batch_size))
# Loop through each batch of file paths
for (i in seq_along(file_batches)) {
cat("Processing batch", i, "of", length(file_batches), "...\n")
# Get the file paths for the current batch
current_batch_files <- file_batches[[i]]
# Use lapply on just this small batch
batch_list <- lapply(current_batch_files, get_events)
batch_list <- batch_list[!sapply(batch_list, is.null)]
if (length(batch_list) > 0) {
batch_dt <- rbindlist(batch_list, fill = TRUE)
# Save the result of this batch to a CSV file
fwrite(batch_dt,
file = "all_events_v2.csv", # <-- FILENAME CHANGED HERE
append = file.exists("all_events_v2.csv"), # <-- FILENAME CHANGED HERE
col.names = !file.exists("all_events_v2.csv")) # <-- FILENAME CHANGED HERE
}
# Optional: Clean up memory before the next batch
rm(batch_list, batch_dt)
gc()
}
cat("Batch processing complete.\n")
# -------------------------------
# 5. Load the final combined CSV
# -------------------------------
print("Loading the final combined dataset from CSV...")
all_events <- fread("all_events_v2.csv") # <-- FILENAME CHANGED HERE
print("CSV file 'all_events_v2.csv' generated successfully!") # <-- FILENAME CHANGED HERE
print("--- Columns ---")
print(colnames(all_events))
print("--- Event types ---")
print(table(all_events$type.name))
